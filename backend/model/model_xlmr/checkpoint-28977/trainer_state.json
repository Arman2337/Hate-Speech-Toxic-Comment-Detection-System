{
  "best_global_step": 28977,
  "best_metric": 0.6782633604574576,
  "best_model_checkpoint": "/content/drive/MyDrive/xlmr-hinglish-toxic/checkpoint-28977",
  "epoch": 3.500060393767363,
  "eval_steps": 500,
  "global_step": 28977,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012078753472641623,
      "grad_norm": 2.811727523803711,
      "learning_rate": 1.9931669945128896e-05,
      "loss": 1.4608,
      "step": 100
    },
    {
      "epoch": 0.024157506945283246,
      "grad_norm": 5.493865489959717,
      "learning_rate": 1.986264968768334e-05,
      "loss": 1.0774,
      "step": 200
    },
    {
      "epoch": 0.03623626041792487,
      "grad_norm": 2.2323834896087646,
      "learning_rate": 1.9793629430237777e-05,
      "loss": 0.6899,
      "step": 300
    },
    {
      "epoch": 0.04831501389056649,
      "grad_norm": 9.735183715820312,
      "learning_rate": 1.9724609172792216e-05,
      "loss": 1.0547,
      "step": 400
    },
    {
      "epoch": 0.06039376736320812,
      "grad_norm": 6.854673385620117,
      "learning_rate": 1.9655588915346655e-05,
      "loss": 0.8161,
      "step": 500
    },
    {
      "epoch": 0.07247252083584974,
      "grad_norm": 10.360896110534668,
      "learning_rate": 1.9586568657901094e-05,
      "loss": 0.6274,
      "step": 600
    },
    {
      "epoch": 0.08455127430849137,
      "grad_norm": 1.5571202039718628,
      "learning_rate": 1.9517548400455536e-05,
      "loss": 0.7484,
      "step": 700
    },
    {
      "epoch": 0.09663002778113298,
      "grad_norm": 60.86893844604492,
      "learning_rate": 1.9448528143009975e-05,
      "loss": 0.7361,
      "step": 800
    },
    {
      "epoch": 0.10870878125377462,
      "grad_norm": 4.816457748413086,
      "learning_rate": 1.9379507885564414e-05,
      "loss": 0.7826,
      "step": 900
    },
    {
      "epoch": 0.12078753472641623,
      "grad_norm": 3.2505874633789062,
      "learning_rate": 1.9310487628118856e-05,
      "loss": 1.1253,
      "step": 1000
    },
    {
      "epoch": 0.13286628819905785,
      "grad_norm": 1.9063482284545898,
      "learning_rate": 1.9241467370673295e-05,
      "loss": 0.8607,
      "step": 1100
    },
    {
      "epoch": 0.14494504167169947,
      "grad_norm": 10.536398887634277,
      "learning_rate": 1.9172447113227734e-05,
      "loss": 0.9232,
      "step": 1200
    },
    {
      "epoch": 0.15702379514434112,
      "grad_norm": 9.115914344787598,
      "learning_rate": 1.9103426855782176e-05,
      "loss": 0.6718,
      "step": 1300
    },
    {
      "epoch": 0.16910254861698273,
      "grad_norm": 3.0850155353546143,
      "learning_rate": 1.9034406598336615e-05,
      "loss": 0.8611,
      "step": 1400
    },
    {
      "epoch": 0.18118130208962435,
      "grad_norm": 83.93938446044922,
      "learning_rate": 1.8965386340891054e-05,
      "loss": 0.8914,
      "step": 1500
    },
    {
      "epoch": 0.19326005556226597,
      "grad_norm": 25.783044815063477,
      "learning_rate": 1.8896366083445493e-05,
      "loss": 0.842,
      "step": 1600
    },
    {
      "epoch": 0.2053388090349076,
      "grad_norm": 1.136440396308899,
      "learning_rate": 1.882734582599993e-05,
      "loss": 0.8462,
      "step": 1700
    },
    {
      "epoch": 0.21741756250754923,
      "grad_norm": 5.445799827575684,
      "learning_rate": 1.875832556855437e-05,
      "loss": 0.5685,
      "step": 1800
    },
    {
      "epoch": 0.22949631598019085,
      "grad_norm": 6.717060089111328,
      "learning_rate": 1.8689305311108813e-05,
      "loss": 0.7259,
      "step": 1900
    },
    {
      "epoch": 0.24157506945283247,
      "grad_norm": 58.94416809082031,
      "learning_rate": 1.862028505366325e-05,
      "loss": 0.9866,
      "step": 2000
    },
    {
      "epoch": 0.2536538229254741,
      "grad_norm": 40.97738265991211,
      "learning_rate": 1.855126479621769e-05,
      "loss": 0.629,
      "step": 2100
    },
    {
      "epoch": 0.2657325763981157,
      "grad_norm": 0.8799219727516174,
      "learning_rate": 1.8482244538772133e-05,
      "loss": 0.6374,
      "step": 2200
    },
    {
      "epoch": 0.27781132987075735,
      "grad_norm": 13.421601295471191,
      "learning_rate": 1.841322428132657e-05,
      "loss": 0.6963,
      "step": 2300
    },
    {
      "epoch": 0.28989008334339894,
      "grad_norm": 3.1679208278656006,
      "learning_rate": 1.834420402388101e-05,
      "loss": 0.5058,
      "step": 2400
    },
    {
      "epoch": 0.3019688368160406,
      "grad_norm": 1.8976774215698242,
      "learning_rate": 1.827518376643545e-05,
      "loss": 1.1844,
      "step": 2500
    },
    {
      "epoch": 0.31404759028868223,
      "grad_norm": 111.25182342529297,
      "learning_rate": 1.8206163508989888e-05,
      "loss": 0.7316,
      "step": 2600
    },
    {
      "epoch": 0.3261263437613238,
      "grad_norm": 27.07158660888672,
      "learning_rate": 1.813714325154433e-05,
      "loss": 0.8856,
      "step": 2700
    },
    {
      "epoch": 0.33820509723396547,
      "grad_norm": 18.75965118408203,
      "learning_rate": 1.806812299409877e-05,
      "loss": 0.7952,
      "step": 2800
    },
    {
      "epoch": 0.35028385070660706,
      "grad_norm": 67.07874298095703,
      "learning_rate": 1.7999102736653208e-05,
      "loss": 0.6576,
      "step": 2900
    },
    {
      "epoch": 0.3623626041792487,
      "grad_norm": 0.8022461533546448,
      "learning_rate": 1.793008247920765e-05,
      "loss": 0.679,
      "step": 3000
    },
    {
      "epoch": 0.37444135765189035,
      "grad_norm": 1.5145549774169922,
      "learning_rate": 1.786106222176209e-05,
      "loss": 0.819,
      "step": 3100
    },
    {
      "epoch": 0.38652011112453194,
      "grad_norm": 3.237766981124878,
      "learning_rate": 1.7792041964316528e-05,
      "loss": 0.6856,
      "step": 3200
    },
    {
      "epoch": 0.3985988645971736,
      "grad_norm": 4.608911991119385,
      "learning_rate": 1.772302170687097e-05,
      "loss": 0.7902,
      "step": 3300
    },
    {
      "epoch": 0.4106776180698152,
      "grad_norm": 2.9733073711395264,
      "learning_rate": 1.765400144942541e-05,
      "loss": 0.7918,
      "step": 3400
    },
    {
      "epoch": 0.4227563715424568,
      "grad_norm": 1.8290976285934448,
      "learning_rate": 1.7584981191979848e-05,
      "loss": 0.4681,
      "step": 3500
    },
    {
      "epoch": 0.43483512501509847,
      "grad_norm": 14.275799751281738,
      "learning_rate": 1.7515960934534287e-05,
      "loss": 0.7758,
      "step": 3600
    },
    {
      "epoch": 0.44691387848774006,
      "grad_norm": 14.445507049560547,
      "learning_rate": 1.7446940677088726e-05,
      "loss": 0.9062,
      "step": 3700
    },
    {
      "epoch": 0.4589926319603817,
      "grad_norm": 14.210243225097656,
      "learning_rate": 1.7377920419643165e-05,
      "loss": 0.6762,
      "step": 3800
    },
    {
      "epoch": 0.4710713854330233,
      "grad_norm": 208.415283203125,
      "learning_rate": 1.7308900162197607e-05,
      "loss": 0.7559,
      "step": 3900
    },
    {
      "epoch": 0.48315013890566494,
      "grad_norm": 1.466751217842102,
      "learning_rate": 1.7239879904752046e-05,
      "loss": 0.6166,
      "step": 4000
    },
    {
      "epoch": 0.4952288923783066,
      "grad_norm": 2.6125874519348145,
      "learning_rate": 1.7170859647306485e-05,
      "loss": 0.495,
      "step": 4100
    },
    {
      "epoch": 0.5073076458509482,
      "grad_norm": 16.570993423461914,
      "learning_rate": 1.7101839389860927e-05,
      "loss": 0.691,
      "step": 4200
    },
    {
      "epoch": 0.5193863993235898,
      "grad_norm": 2.3323984146118164,
      "learning_rate": 1.7032819132415366e-05,
      "loss": 0.8326,
      "step": 4300
    },
    {
      "epoch": 0.5314651527962314,
      "grad_norm": 4.479079246520996,
      "learning_rate": 1.6963798874969804e-05,
      "loss": 0.857,
      "step": 4400
    },
    {
      "epoch": 0.5435439062688731,
      "grad_norm": 13.902413368225098,
      "learning_rate": 1.6894778617524247e-05,
      "loss": 0.8057,
      "step": 4500
    },
    {
      "epoch": 0.5556226597415147,
      "grad_norm": 7.518765449523926,
      "learning_rate": 1.6825758360078686e-05,
      "loss": 0.7286,
      "step": 4600
    },
    {
      "epoch": 0.5677014132141563,
      "grad_norm": 7.303544521331787,
      "learning_rate": 1.6756738102633124e-05,
      "loss": 0.6176,
      "step": 4700
    },
    {
      "epoch": 0.5797801666867979,
      "grad_norm": 0.18455956876277924,
      "learning_rate": 1.6687717845187563e-05,
      "loss": 0.6357,
      "step": 4800
    },
    {
      "epoch": 0.5918589201594395,
      "grad_norm": 2.120863676071167,
      "learning_rate": 1.6618697587742002e-05,
      "loss": 0.5198,
      "step": 4900
    },
    {
      "epoch": 0.6039376736320812,
      "grad_norm": 2.302090883255005,
      "learning_rate": 1.6549677330296444e-05,
      "loss": 0.504,
      "step": 5000
    },
    {
      "epoch": 0.6160164271047228,
      "grad_norm": 1.9059494733810425,
      "learning_rate": 1.6480657072850883e-05,
      "loss": 0.5073,
      "step": 5100
    },
    {
      "epoch": 0.6280951805773645,
      "grad_norm": 60.22712707519531,
      "learning_rate": 1.6411636815405322e-05,
      "loss": 0.65,
      "step": 5200
    },
    {
      "epoch": 0.640173934050006,
      "grad_norm": 84.27749633789062,
      "learning_rate": 1.6342616557959764e-05,
      "loss": 0.5874,
      "step": 5300
    },
    {
      "epoch": 0.6522526875226476,
      "grad_norm": 27.931116104125977,
      "learning_rate": 1.6273596300514203e-05,
      "loss": 0.6756,
      "step": 5400
    },
    {
      "epoch": 0.6643314409952893,
      "grad_norm": 35.9768180847168,
      "learning_rate": 1.6204576043068642e-05,
      "loss": 0.8333,
      "step": 5500
    },
    {
      "epoch": 0.6764101944679309,
      "grad_norm": 660.8250732421875,
      "learning_rate": 1.613555578562308e-05,
      "loss": 0.4563,
      "step": 5600
    },
    {
      "epoch": 0.6884889479405726,
      "grad_norm": 1.4831990003585815,
      "learning_rate": 1.606653552817752e-05,
      "loss": 0.6657,
      "step": 5700
    },
    {
      "epoch": 0.7005677014132141,
      "grad_norm": 1.353029727935791,
      "learning_rate": 1.599751527073196e-05,
      "loss": 0.7002,
      "step": 5800
    },
    {
      "epoch": 0.7126464548858558,
      "grad_norm": 104.1611557006836,
      "learning_rate": 1.59284950132864e-05,
      "loss": 0.381,
      "step": 5900
    },
    {
      "epoch": 0.7247252083584974,
      "grad_norm": 35.44517517089844,
      "learning_rate": 1.585947475584084e-05,
      "loss": 0.4132,
      "step": 6000
    },
    {
      "epoch": 0.736803961831139,
      "grad_norm": 4.4141340255737305,
      "learning_rate": 1.579045449839528e-05,
      "loss": 0.6141,
      "step": 6100
    },
    {
      "epoch": 0.7488827153037807,
      "grad_norm": 5.771075248718262,
      "learning_rate": 1.572143424094972e-05,
      "loss": 0.9919,
      "step": 6200
    },
    {
      "epoch": 0.7609614687764222,
      "grad_norm": 2.206489324569702,
      "learning_rate": 1.565241398350416e-05,
      "loss": 0.7591,
      "step": 6300
    },
    {
      "epoch": 0.7730402222490639,
      "grad_norm": 21.901647567749023,
      "learning_rate": 1.55833937260586e-05,
      "loss": 0.9491,
      "step": 6400
    },
    {
      "epoch": 0.7851189757217055,
      "grad_norm": 7.886033535003662,
      "learning_rate": 1.551437346861304e-05,
      "loss": 0.4695,
      "step": 6500
    },
    {
      "epoch": 0.7971977291943472,
      "grad_norm": 206.6540069580078,
      "learning_rate": 1.544535321116748e-05,
      "loss": 0.53,
      "step": 6600
    },
    {
      "epoch": 0.8092764826669888,
      "grad_norm": 524.005126953125,
      "learning_rate": 1.537633295372192e-05,
      "loss": 0.7549,
      "step": 6700
    },
    {
      "epoch": 0.8213552361396304,
      "grad_norm": 22.988903045654297,
      "learning_rate": 1.5307312696276357e-05,
      "loss": 0.8148,
      "step": 6800
    },
    {
      "epoch": 0.833433989612272,
      "grad_norm": 5.165746212005615,
      "learning_rate": 1.5238292438830798e-05,
      "loss": 0.4974,
      "step": 6900
    },
    {
      "epoch": 0.8455127430849136,
      "grad_norm": 4.327621936798096,
      "learning_rate": 1.5169272181385237e-05,
      "loss": 0.7537,
      "step": 7000
    },
    {
      "epoch": 0.8575914965575553,
      "grad_norm": 34.5125617980957,
      "learning_rate": 1.5100251923939677e-05,
      "loss": 0.9499,
      "step": 7100
    },
    {
      "epoch": 0.8696702500301969,
      "grad_norm": 7.604310989379883,
      "learning_rate": 1.5031231666494116e-05,
      "loss": 0.6871,
      "step": 7200
    },
    {
      "epoch": 0.8817490035028385,
      "grad_norm": 93.64720153808594,
      "learning_rate": 1.4962211409048557e-05,
      "loss": 0.684,
      "step": 7300
    },
    {
      "epoch": 0.8938277569754801,
      "grad_norm": 12.587818145751953,
      "learning_rate": 1.4893191151602997e-05,
      "loss": 0.8337,
      "step": 7400
    },
    {
      "epoch": 0.9059065104481218,
      "grad_norm": 20.16465187072754,
      "learning_rate": 1.4824170894157436e-05,
      "loss": 0.7333,
      "step": 7500
    },
    {
      "epoch": 0.9179852639207634,
      "grad_norm": 12.128778457641602,
      "learning_rate": 1.4755150636711877e-05,
      "loss": 0.5416,
      "step": 7600
    },
    {
      "epoch": 0.930064017393405,
      "grad_norm": 1.1289582252502441,
      "learning_rate": 1.4686130379266317e-05,
      "loss": 0.3741,
      "step": 7700
    },
    {
      "epoch": 0.9421427708660466,
      "grad_norm": 123.55087280273438,
      "learning_rate": 1.4617110121820755e-05,
      "loss": 0.8883,
      "step": 7800
    },
    {
      "epoch": 0.9542215243386882,
      "grad_norm": 8.360321044921875,
      "learning_rate": 1.4548089864375195e-05,
      "loss": 0.3277,
      "step": 7900
    },
    {
      "epoch": 0.9663002778113299,
      "grad_norm": 13.286774635314941,
      "learning_rate": 1.4479069606929634e-05,
      "loss": 0.3343,
      "step": 8000
    },
    {
      "epoch": 0.9783790312839715,
      "grad_norm": 1.34298837184906,
      "learning_rate": 1.4410049349484075e-05,
      "loss": 0.5163,
      "step": 8100
    },
    {
      "epoch": 0.9904577847566132,
      "grad_norm": 7.074621200561523,
      "learning_rate": 1.4341029092038513e-05,
      "loss": 0.8853,
      "step": 8200
    },
    {
      "epoch": 1.0,
      "eval_f1_macro": 0.5453326083598458,
      "eval_f1_micro": 0.6947314049586777,
      "eval_loss": 0.5798635482788086,
      "eval_roc_auc_macro": 0.9788086655427893,
      "eval_runtime": 27.7317,
      "eval_samples_per_second": 597.042,
      "eval_steps_per_second": 18.679,
      "step": 8279
    },
    {
      "epoch": 1.0025365382292548,
      "grad_norm": 23.485273361206055,
      "learning_rate": 1.4272008834592954e-05,
      "loss": 0.3886,
      "step": 8300
    },
    {
      "epoch": 1.0146152917018965,
      "grad_norm": 3.4697062969207764,
      "learning_rate": 1.4202988577147395e-05,
      "loss": 0.4436,
      "step": 8400
    },
    {
      "epoch": 1.0266940451745379,
      "grad_norm": 9.298535346984863,
      "learning_rate": 1.4133968319701833e-05,
      "loss": 0.3604,
      "step": 8500
    },
    {
      "epoch": 1.0387727986471795,
      "grad_norm": 6.284457683563232,
      "learning_rate": 1.4064948062256274e-05,
      "loss": 0.4662,
      "step": 8600
    },
    {
      "epoch": 1.0508515521198212,
      "grad_norm": 19.7611141204834,
      "learning_rate": 1.3995927804810714e-05,
      "loss": 0.6855,
      "step": 8700
    },
    {
      "epoch": 1.0629303055924628,
      "grad_norm": 3.041285991668701,
      "learning_rate": 1.3926907547365153e-05,
      "loss": 0.3319,
      "step": 8800
    },
    {
      "epoch": 1.0750090590651045,
      "grad_norm": 30.12917709350586,
      "learning_rate": 1.3857887289919592e-05,
      "loss": 0.2797,
      "step": 8900
    },
    {
      "epoch": 1.0870878125377461,
      "grad_norm": 159.21456909179688,
      "learning_rate": 1.3788867032474031e-05,
      "loss": 0.7152,
      "step": 9000
    },
    {
      "epoch": 1.0991665660103878,
      "grad_norm": 0.8356517553329468,
      "learning_rate": 1.3719846775028472e-05,
      "loss": 0.5382,
      "step": 9100
    },
    {
      "epoch": 1.1112453194830294,
      "grad_norm": 0.8343092799186707,
      "learning_rate": 1.3650826517582912e-05,
      "loss": 0.49,
      "step": 9200
    },
    {
      "epoch": 1.123324072955671,
      "grad_norm": 1.5978718996047974,
      "learning_rate": 1.3581806260137351e-05,
      "loss": 0.5366,
      "step": 9300
    },
    {
      "epoch": 1.1354028264283127,
      "grad_norm": 43.14076232910156,
      "learning_rate": 1.3512786002691792e-05,
      "loss": 0.6288,
      "step": 9400
    },
    {
      "epoch": 1.1474815799009543,
      "grad_norm": 1.380528450012207,
      "learning_rate": 1.344376574524623e-05,
      "loss": 0.668,
      "step": 9500
    },
    {
      "epoch": 1.1595603333735958,
      "grad_norm": 2.6518566608428955,
      "learning_rate": 1.3374745487800671e-05,
      "loss": 0.7588,
      "step": 9600
    },
    {
      "epoch": 1.1716390868462374,
      "grad_norm": 2.588168144226074,
      "learning_rate": 1.3305725230355112e-05,
      "loss": 0.7612,
      "step": 9700
    },
    {
      "epoch": 1.183717840318879,
      "grad_norm": 50.32748794555664,
      "learning_rate": 1.323670497290955e-05,
      "loss": 0.3606,
      "step": 9800
    },
    {
      "epoch": 1.1957965937915207,
      "grad_norm": 3.6908416748046875,
      "learning_rate": 1.3167684715463991e-05,
      "loss": 0.5365,
      "step": 9900
    },
    {
      "epoch": 1.2078753472641623,
      "grad_norm": 3.2523765563964844,
      "learning_rate": 1.3098664458018428e-05,
      "loss": 0.559,
      "step": 10000
    },
    {
      "epoch": 1.219954100736804,
      "grad_norm": 29.434661865234375,
      "learning_rate": 1.3029644200572869e-05,
      "loss": 0.475,
      "step": 10100
    },
    {
      "epoch": 1.2320328542094456,
      "grad_norm": 11.198877334594727,
      "learning_rate": 1.296062394312731e-05,
      "loss": 0.39,
      "step": 10200
    },
    {
      "epoch": 1.2441116076820873,
      "grad_norm": 5.38131046295166,
      "learning_rate": 1.2891603685681748e-05,
      "loss": 0.8544,
      "step": 10300
    },
    {
      "epoch": 1.256190361154729,
      "grad_norm": 4.055849552154541,
      "learning_rate": 1.2822583428236189e-05,
      "loss": 0.6092,
      "step": 10400
    },
    {
      "epoch": 1.2682691146273704,
      "grad_norm": 3.4714462757110596,
      "learning_rate": 1.2753563170790628e-05,
      "loss": 0.6426,
      "step": 10500
    },
    {
      "epoch": 1.280347868100012,
      "grad_norm": 0.07922432571649551,
      "learning_rate": 1.2684542913345068e-05,
      "loss": 0.5203,
      "step": 10600
    },
    {
      "epoch": 1.2924266215726536,
      "grad_norm": 3.3011534214019775,
      "learning_rate": 1.2615522655899509e-05,
      "loss": 0.6699,
      "step": 10700
    },
    {
      "epoch": 1.3045053750452953,
      "grad_norm": 19.122909545898438,
      "learning_rate": 1.2546502398453947e-05,
      "loss": 0.5323,
      "step": 10800
    },
    {
      "epoch": 1.316584128517937,
      "grad_norm": 6.968634605407715,
      "learning_rate": 1.2477482141008388e-05,
      "loss": 0.5171,
      "step": 10900
    },
    {
      "epoch": 1.3286628819905786,
      "grad_norm": 6.109159469604492,
      "learning_rate": 1.2408461883562825e-05,
      "loss": 0.7431,
      "step": 11000
    },
    {
      "epoch": 1.3407416354632202,
      "grad_norm": 1.8464888334274292,
      "learning_rate": 1.2339441626117266e-05,
      "loss": 0.314,
      "step": 11100
    },
    {
      "epoch": 1.3528203889358619,
      "grad_norm": 4.292521953582764,
      "learning_rate": 1.2270421368671706e-05,
      "loss": 0.9075,
      "step": 11200
    },
    {
      "epoch": 1.3648991424085035,
      "grad_norm": 139.06369018554688,
      "learning_rate": 1.2201401111226145e-05,
      "loss": 0.8631,
      "step": 11300
    },
    {
      "epoch": 1.376977895881145,
      "grad_norm": 46.7941780090332,
      "learning_rate": 1.2132380853780586e-05,
      "loss": 0.702,
      "step": 11400
    },
    {
      "epoch": 1.3890566493537868,
      "grad_norm": 3.376232624053955,
      "learning_rate": 1.2063360596335025e-05,
      "loss": 0.5332,
      "step": 11500
    },
    {
      "epoch": 1.4011354028264282,
      "grad_norm": 4.0571722984313965,
      "learning_rate": 1.1994340338889465e-05,
      "loss": 0.3919,
      "step": 11600
    },
    {
      "epoch": 1.4132141562990699,
      "grad_norm": 3.5647733211517334,
      "learning_rate": 1.1925320081443906e-05,
      "loss": 0.5475,
      "step": 11700
    },
    {
      "epoch": 1.4252929097717115,
      "grad_norm": 18.62343406677246,
      "learning_rate": 1.1856299823998345e-05,
      "loss": 0.337,
      "step": 11800
    },
    {
      "epoch": 1.4373716632443532,
      "grad_norm": 7.077512264251709,
      "learning_rate": 1.1787279566552785e-05,
      "loss": 0.6389,
      "step": 11900
    },
    {
      "epoch": 1.4494504167169948,
      "grad_norm": 1.2715479135513306,
      "learning_rate": 1.1718259309107226e-05,
      "loss": 0.4722,
      "step": 12000
    },
    {
      "epoch": 1.4615291701896365,
      "grad_norm": 1.2681736946105957,
      "learning_rate": 1.1649239051661663e-05,
      "loss": 0.4032,
      "step": 12100
    },
    {
      "epoch": 1.473607923662278,
      "grad_norm": 9.165885925292969,
      "learning_rate": 1.1580218794216103e-05,
      "loss": 0.6046,
      "step": 12200
    },
    {
      "epoch": 1.4856866771349198,
      "grad_norm": 10.183395385742188,
      "learning_rate": 1.1511198536770542e-05,
      "loss": 0.4055,
      "step": 12300
    },
    {
      "epoch": 1.4977654306075614,
      "grad_norm": 5.632758140563965,
      "learning_rate": 1.1442178279324983e-05,
      "loss": 0.6724,
      "step": 12400
    },
    {
      "epoch": 1.5098441840802028,
      "grad_norm": 1.6817152500152588,
      "learning_rate": 1.1373158021879422e-05,
      "loss": 0.5944,
      "step": 12500
    },
    {
      "epoch": 1.5219229375528447,
      "grad_norm": 10.931950569152832,
      "learning_rate": 1.1304137764433862e-05,
      "loss": 0.4255,
      "step": 12600
    },
    {
      "epoch": 1.5340016910254861,
      "grad_norm": 3.5825417041778564,
      "learning_rate": 1.1235117506988303e-05,
      "loss": 0.6199,
      "step": 12700
    },
    {
      "epoch": 1.5460804444981278,
      "grad_norm": 8.732068061828613,
      "learning_rate": 1.1166097249542742e-05,
      "loss": 0.3922,
      "step": 12800
    },
    {
      "epoch": 1.5581591979707694,
      "grad_norm": 5.683261394500732,
      "learning_rate": 1.1097076992097182e-05,
      "loss": 0.4352,
      "step": 12900
    },
    {
      "epoch": 1.570237951443411,
      "grad_norm": 22.107751846313477,
      "learning_rate": 1.1028056734651623e-05,
      "loss": 0.3193,
      "step": 13000
    },
    {
      "epoch": 1.5823167049160527,
      "grad_norm": 3.255657911300659,
      "learning_rate": 1.095903647720606e-05,
      "loss": 0.5497,
      "step": 13100
    },
    {
      "epoch": 1.5943954583886943,
      "grad_norm": 25.639333724975586,
      "learning_rate": 1.08900162197605e-05,
      "loss": 0.6968,
      "step": 13200
    },
    {
      "epoch": 1.606474211861336,
      "grad_norm": 16.826080322265625,
      "learning_rate": 1.082099596231494e-05,
      "loss": 0.563,
      "step": 13300
    },
    {
      "epoch": 1.6185529653339774,
      "grad_norm": 5.099236011505127,
      "learning_rate": 1.075197570486938e-05,
      "loss": 0.6319,
      "step": 13400
    },
    {
      "epoch": 1.6306317188066193,
      "grad_norm": 8.483757019042969,
      "learning_rate": 1.0682955447423819e-05,
      "loss": 0.3765,
      "step": 13500
    },
    {
      "epoch": 1.6427104722792607,
      "grad_norm": 42.74416732788086,
      "learning_rate": 1.061393518997826e-05,
      "loss": 0.477,
      "step": 13600
    },
    {
      "epoch": 1.6547892257519023,
      "grad_norm": 10.830344200134277,
      "learning_rate": 1.05449149325327e-05,
      "loss": 0.4566,
      "step": 13700
    },
    {
      "epoch": 1.666867979224544,
      "grad_norm": 7.872790813446045,
      "learning_rate": 1.0475894675087139e-05,
      "loss": 0.3886,
      "step": 13800
    },
    {
      "epoch": 1.6789467326971856,
      "grad_norm": 5.478860855102539,
      "learning_rate": 1.040687441764158e-05,
      "loss": 0.3645,
      "step": 13900
    },
    {
      "epoch": 1.6910254861698273,
      "grad_norm": 3.648383378982544,
      "learning_rate": 1.033785416019602e-05,
      "loss": 0.4648,
      "step": 14000
    },
    {
      "epoch": 1.703104239642469,
      "grad_norm": 2.425135374069214,
      "learning_rate": 1.0268833902750459e-05,
      "loss": 0.5549,
      "step": 14100
    },
    {
      "epoch": 1.7151829931151106,
      "grad_norm": 4.349136829376221,
      "learning_rate": 1.0199813645304898e-05,
      "loss": 0.6598,
      "step": 14200
    },
    {
      "epoch": 1.727261746587752,
      "grad_norm": 9.155823707580566,
      "learning_rate": 1.0130793387859336e-05,
      "loss": 0.4216,
      "step": 14300
    },
    {
      "epoch": 1.7393405000603939,
      "grad_norm": 6.207027435302734,
      "learning_rate": 1.0061773130413777e-05,
      "loss": 0.6175,
      "step": 14400
    },
    {
      "epoch": 1.7514192535330353,
      "grad_norm": 2.6392955780029297,
      "learning_rate": 9.992752872968216e-06,
      "loss": 0.5317,
      "step": 14500
    },
    {
      "epoch": 1.7634980070056772,
      "grad_norm": 3.4144656658172607,
      "learning_rate": 9.923732615522656e-06,
      "loss": 0.654,
      "step": 14600
    },
    {
      "epoch": 1.7755767604783186,
      "grad_norm": 1.6928240060806274,
      "learning_rate": 9.854712358077097e-06,
      "loss": 0.3842,
      "step": 14700
    },
    {
      "epoch": 1.7876555139509602,
      "grad_norm": 1.0190324783325195,
      "learning_rate": 9.785692100631536e-06,
      "loss": 0.3946,
      "step": 14800
    },
    {
      "epoch": 1.7997342674236019,
      "grad_norm": 7.511749744415283,
      "learning_rate": 9.716671843185976e-06,
      "loss": 0.5,
      "step": 14900
    },
    {
      "epoch": 1.8118130208962435,
      "grad_norm": 129.52223205566406,
      "learning_rate": 9.647651585740415e-06,
      "loss": 0.3652,
      "step": 15000
    },
    {
      "epoch": 1.8238917743688852,
      "grad_norm": 1.634583830833435,
      "learning_rate": 9.578631328294856e-06,
      "loss": 0.6092,
      "step": 15100
    },
    {
      "epoch": 1.8359705278415266,
      "grad_norm": 2.045774221420288,
      "learning_rate": 9.509611070849295e-06,
      "loss": 0.4013,
      "step": 15200
    },
    {
      "epoch": 1.8480492813141685,
      "grad_norm": 5.789734363555908,
      "learning_rate": 9.440590813403735e-06,
      "loss": 0.4204,
      "step": 15300
    },
    {
      "epoch": 1.8601280347868099,
      "grad_norm": 1.817325472831726,
      "learning_rate": 9.371570555958174e-06,
      "loss": 0.5278,
      "step": 15400
    },
    {
      "epoch": 1.8722067882594517,
      "grad_norm": 128.94166564941406,
      "learning_rate": 9.302550298512613e-06,
      "loss": 0.7095,
      "step": 15500
    },
    {
      "epoch": 1.8842855417320932,
      "grad_norm": 59.8837890625,
      "learning_rate": 9.233530041067053e-06,
      "loss": 0.6136,
      "step": 15600
    },
    {
      "epoch": 1.8963642952047348,
      "grad_norm": 11.582252502441406,
      "learning_rate": 9.164509783621494e-06,
      "loss": 0.3535,
      "step": 15700
    },
    {
      "epoch": 1.9084430486773765,
      "grad_norm": 4.257127285003662,
      "learning_rate": 9.095489526175933e-06,
      "loss": 0.3163,
      "step": 15800
    },
    {
      "epoch": 1.920521802150018,
      "grad_norm": 2.2631547451019287,
      "learning_rate": 9.026469268730373e-06,
      "loss": 0.6065,
      "step": 15900
    },
    {
      "epoch": 1.9326005556226598,
      "grad_norm": 367.939453125,
      "learning_rate": 8.957449011284814e-06,
      "loss": 0.5795,
      "step": 16000
    },
    {
      "epoch": 1.9446793090953014,
      "grad_norm": 174.4746856689453,
      "learning_rate": 8.888428753839253e-06,
      "loss": 0.6839,
      "step": 16100
    },
    {
      "epoch": 1.956758062567943,
      "grad_norm": 4.8257527351379395,
      "learning_rate": 8.819408496393692e-06,
      "loss": 0.5444,
      "step": 16200
    },
    {
      "epoch": 1.9688368160405845,
      "grad_norm": 0.2666604518890381,
      "learning_rate": 8.750388238948132e-06,
      "loss": 0.3904,
      "step": 16300
    },
    {
      "epoch": 1.9809155695132263,
      "grad_norm": 3.0622832775115967,
      "learning_rate": 8.681367981502573e-06,
      "loss": 0.8268,
      "step": 16400
    },
    {
      "epoch": 1.9929943229858678,
      "grad_norm": 1.8916115760803223,
      "learning_rate": 8.612347724057012e-06,
      "loss": 0.458,
      "step": 16500
    },
    {
      "epoch": 2.0,
      "eval_f1_macro": 0.6431810390521785,
      "eval_f1_micro": 0.760950472373318,
      "eval_loss": 0.5296598076820374,
      "eval_roc_auc_macro": 0.9843217087353892,
      "eval_runtime": 27.5389,
      "eval_samples_per_second": 601.223,
      "eval_steps_per_second": 18.81,
      "step": 16558
    },
    {
      "epoch": 2.0050730764585096,
      "grad_norm": 1.5990948677062988,
      "learning_rate": 8.54332746661145e-06,
      "loss": 0.4651,
      "step": 16600
    },
    {
      "epoch": 2.017151829931151,
      "grad_norm": 3.5846810340881348,
      "learning_rate": 8.474307209165891e-06,
      "loss": 0.5384,
      "step": 16700
    },
    {
      "epoch": 2.029230583403793,
      "grad_norm": 15.782219886779785,
      "learning_rate": 8.40528695172033e-06,
      "loss": 0.5862,
      "step": 16800
    },
    {
      "epoch": 2.0413093368764343,
      "grad_norm": 103.55352783203125,
      "learning_rate": 8.33626669427477e-06,
      "loss": 0.4886,
      "step": 16900
    },
    {
      "epoch": 2.0533880903490758,
      "grad_norm": 0.14858750998973846,
      "learning_rate": 8.267246436829211e-06,
      "loss": 0.4167,
      "step": 17000
    },
    {
      "epoch": 2.0654668438217176,
      "grad_norm": 70.66353607177734,
      "learning_rate": 8.19822617938365e-06,
      "loss": 0.4097,
      "step": 17100
    },
    {
      "epoch": 2.077545597294359,
      "grad_norm": 5.160285472869873,
      "learning_rate": 8.129205921938089e-06,
      "loss": 0.4356,
      "step": 17200
    },
    {
      "epoch": 2.089624350767001,
      "grad_norm": 42.355716705322266,
      "learning_rate": 8.06018566449253e-06,
      "loss": 0.4035,
      "step": 17300
    },
    {
      "epoch": 2.1017031042396424,
      "grad_norm": 218.25775146484375,
      "learning_rate": 7.99116540704697e-06,
      "loss": 0.4112,
      "step": 17400
    },
    {
      "epoch": 2.113781857712284,
      "grad_norm": 3.8981850147247314,
      "learning_rate": 7.922145149601409e-06,
      "loss": 0.2654,
      "step": 17500
    },
    {
      "epoch": 2.1258606111849256,
      "grad_norm": 4.920993328094482,
      "learning_rate": 7.853124892155848e-06,
      "loss": 0.4229,
      "step": 17600
    },
    {
      "epoch": 2.1379393646575675,
      "grad_norm": 7.144382953643799,
      "learning_rate": 7.784104634710288e-06,
      "loss": 0.4029,
      "step": 17700
    },
    {
      "epoch": 2.150018118130209,
      "grad_norm": 1.0165349245071411,
      "learning_rate": 7.715084377264727e-06,
      "loss": 0.5053,
      "step": 17800
    },
    {
      "epoch": 2.1620968716028504,
      "grad_norm": 17.62403678894043,
      "learning_rate": 7.646064119819168e-06,
      "loss": 0.5164,
      "step": 17900
    },
    {
      "epoch": 2.1741756250754922,
      "grad_norm": 2.395019769668579,
      "learning_rate": 7.577043862373607e-06,
      "loss": 0.2756,
      "step": 18000
    },
    {
      "epoch": 2.1862543785481336,
      "grad_norm": 6.096604824066162,
      "learning_rate": 7.508023604928048e-06,
      "loss": 0.4115,
      "step": 18100
    },
    {
      "epoch": 2.1983331320207755,
      "grad_norm": 5.379556655883789,
      "learning_rate": 7.439003347482487e-06,
      "loss": 0.5079,
      "step": 18200
    },
    {
      "epoch": 2.210411885493417,
      "grad_norm": 2.9607291221618652,
      "learning_rate": 7.369983090036926e-06,
      "loss": 0.4241,
      "step": 18300
    },
    {
      "epoch": 2.222490638966059,
      "grad_norm": 12.469078063964844,
      "learning_rate": 7.300962832591366e-06,
      "loss": 0.271,
      "step": 18400
    },
    {
      "epoch": 2.2345693924387002,
      "grad_norm": 18.54796600341797,
      "learning_rate": 7.231942575145806e-06,
      "loss": 0.6555,
      "step": 18500
    },
    {
      "epoch": 2.246648145911342,
      "grad_norm": 115.9103012084961,
      "learning_rate": 7.162922317700246e-06,
      "loss": 0.3547,
      "step": 18600
    },
    {
      "epoch": 2.2587268993839835,
      "grad_norm": 1.1612625122070312,
      "learning_rate": 7.093902060254685e-06,
      "loss": 0.3987,
      "step": 18700
    },
    {
      "epoch": 2.2708056528566254,
      "grad_norm": 1.065678358078003,
      "learning_rate": 7.024881802809125e-06,
      "loss": 0.1695,
      "step": 18800
    },
    {
      "epoch": 2.282884406329267,
      "grad_norm": 2.214139938354492,
      "learning_rate": 6.955861545363565e-06,
      "loss": 0.4104,
      "step": 18900
    },
    {
      "epoch": 2.2949631598019087,
      "grad_norm": 9.332352638244629,
      "learning_rate": 6.886841287918004e-06,
      "loss": 0.3491,
      "step": 19000
    },
    {
      "epoch": 2.30704191327455,
      "grad_norm": 2.301027774810791,
      "learning_rate": 6.817821030472445e-06,
      "loss": 0.3264,
      "step": 19100
    },
    {
      "epoch": 2.3191206667471915,
      "grad_norm": 10.256730079650879,
      "learning_rate": 6.748800773026884e-06,
      "loss": 0.8141,
      "step": 19200
    },
    {
      "epoch": 2.3311994202198334,
      "grad_norm": 1636.990478515625,
      "learning_rate": 6.6797805155813235e-06,
      "loss": 0.4339,
      "step": 19300
    },
    {
      "epoch": 2.343278173692475,
      "grad_norm": 11.329752922058105,
      "learning_rate": 6.610760258135763e-06,
      "loss": 0.4322,
      "step": 19400
    },
    {
      "epoch": 2.3553569271651167,
      "grad_norm": 3.203522205352783,
      "learning_rate": 6.541740000690203e-06,
      "loss": 0.4529,
      "step": 19500
    },
    {
      "epoch": 2.367435680637758,
      "grad_norm": 12.417237281799316,
      "learning_rate": 6.4727197432446435e-06,
      "loss": 0.4667,
      "step": 19600
    },
    {
      "epoch": 2.3795144341104,
      "grad_norm": 3.794236421585083,
      "learning_rate": 6.403699485799083e-06,
      "loss": 0.2659,
      "step": 19700
    },
    {
      "epoch": 2.3915931875830414,
      "grad_norm": 1.2791835069656372,
      "learning_rate": 6.334679228353522e-06,
      "loss": 0.3261,
      "step": 19800
    },
    {
      "epoch": 2.4036719410556833,
      "grad_norm": 1.6900614500045776,
      "learning_rate": 6.265658970907962e-06,
      "loss": 0.4154,
      "step": 19900
    },
    {
      "epoch": 2.4157506945283247,
      "grad_norm": 2.56622314453125,
      "learning_rate": 6.1966387134624015e-06,
      "loss": 0.4775,
      "step": 20000
    },
    {
      "epoch": 2.427829448000966,
      "grad_norm": 6.642101764678955,
      "learning_rate": 6.127618456016842e-06,
      "loss": 0.3165,
      "step": 20100
    },
    {
      "epoch": 2.439908201473608,
      "grad_norm": 1.4029967784881592,
      "learning_rate": 6.058598198571282e-06,
      "loss": 0.2129,
      "step": 20200
    },
    {
      "epoch": 2.4519869549462494,
      "grad_norm": 21.440601348876953,
      "learning_rate": 5.989577941125721e-06,
      "loss": 0.3114,
      "step": 20300
    },
    {
      "epoch": 2.4640657084188913,
      "grad_norm": 17.311603546142578,
      "learning_rate": 5.92055768368016e-06,
      "loss": 0.4406,
      "step": 20400
    },
    {
      "epoch": 2.4761444618915327,
      "grad_norm": 1.9630911350250244,
      "learning_rate": 5.8515374262346e-06,
      "loss": 0.446,
      "step": 20500
    },
    {
      "epoch": 2.4882232153641746,
      "grad_norm": 2.4423906803131104,
      "learning_rate": 5.7825171687890405e-06,
      "loss": 0.4441,
      "step": 20600
    },
    {
      "epoch": 2.500301968836816,
      "grad_norm": 1.6191153526306152,
      "learning_rate": 5.71349691134348e-06,
      "loss": 0.1916,
      "step": 20700
    },
    {
      "epoch": 2.512380722309458,
      "grad_norm": 1.6704031229019165,
      "learning_rate": 5.644476653897919e-06,
      "loss": 0.4818,
      "step": 20800
    },
    {
      "epoch": 2.5244594757820993,
      "grad_norm": 104.36708068847656,
      "learning_rate": 5.575456396452359e-06,
      "loss": 0.4316,
      "step": 20900
    },
    {
      "epoch": 2.5365382292547407,
      "grad_norm": 2.1954827308654785,
      "learning_rate": 5.5064361390067985e-06,
      "loss": 0.4605,
      "step": 21000
    },
    {
      "epoch": 2.5486169827273826,
      "grad_norm": 49.13093185424805,
      "learning_rate": 5.437415881561239e-06,
      "loss": 0.4311,
      "step": 21100
    },
    {
      "epoch": 2.560695736200024,
      "grad_norm": 5.837085723876953,
      "learning_rate": 5.368395624115679e-06,
      "loss": 0.2569,
      "step": 21200
    },
    {
      "epoch": 2.572774489672666,
      "grad_norm": 2.019477605819702,
      "learning_rate": 5.2993753666701185e-06,
      "loss": 0.3706,
      "step": 21300
    },
    {
      "epoch": 2.5848532431453073,
      "grad_norm": 2.1378166675567627,
      "learning_rate": 5.230355109224557e-06,
      "loss": 0.3528,
      "step": 21400
    },
    {
      "epoch": 2.596931996617949,
      "grad_norm": 0.8186789751052856,
      "learning_rate": 5.161334851778997e-06,
      "loss": 0.4652,
      "step": 21500
    },
    {
      "epoch": 2.6090107500905906,
      "grad_norm": 2.343848466873169,
      "learning_rate": 5.092314594333438e-06,
      "loss": 0.3005,
      "step": 21600
    },
    {
      "epoch": 2.6210895035632324,
      "grad_norm": 7.74116849899292,
      "learning_rate": 5.023294336887877e-06,
      "loss": 0.4874,
      "step": 21700
    },
    {
      "epoch": 2.633168257035874,
      "grad_norm": 0.0367182232439518,
      "learning_rate": 4.954274079442317e-06,
      "loss": 0.5191,
      "step": 21800
    },
    {
      "epoch": 2.6452470105085153,
      "grad_norm": 2.2950754165649414,
      "learning_rate": 4.885253821996757e-06,
      "loss": 0.2365,
      "step": 21900
    },
    {
      "epoch": 2.657325763981157,
      "grad_norm": 79.28003692626953,
      "learning_rate": 4.816233564551196e-06,
      "loss": 0.4575,
      "step": 22000
    },
    {
      "epoch": 2.669404517453799,
      "grad_norm": 42.31231689453125,
      "learning_rate": 4.747213307105636e-06,
      "loss": 0.4224,
      "step": 22100
    },
    {
      "epoch": 2.6814832709264405,
      "grad_norm": 330.5805358886719,
      "learning_rate": 4.678193049660076e-06,
      "loss": 0.683,
      "step": 22200
    },
    {
      "epoch": 2.693562024399082,
      "grad_norm": 11.395047187805176,
      "learning_rate": 4.609172792214516e-06,
      "loss": 0.3803,
      "step": 22300
    },
    {
      "epoch": 2.7056407778717237,
      "grad_norm": 16.536500930786133,
      "learning_rate": 4.540152534768955e-06,
      "loss": 0.4031,
      "step": 22400
    },
    {
      "epoch": 2.717719531344365,
      "grad_norm": 2.5823493003845215,
      "learning_rate": 4.471132277323395e-06,
      "loss": 0.7678,
      "step": 22500
    },
    {
      "epoch": 2.729798284817007,
      "grad_norm": 3.001957893371582,
      "learning_rate": 4.402112019877835e-06,
      "loss": 0.2218,
      "step": 22600
    },
    {
      "epoch": 2.7418770382896485,
      "grad_norm": 0.05713937059044838,
      "learning_rate": 4.333091762432274e-06,
      "loss": 0.2546,
      "step": 22700
    },
    {
      "epoch": 2.75395579176229,
      "grad_norm": 20.84449005126953,
      "learning_rate": 4.264071504986714e-06,
      "loss": 0.5732,
      "step": 22800
    },
    {
      "epoch": 2.7660345452349318,
      "grad_norm": 6.510447978973389,
      "learning_rate": 4.195051247541154e-06,
      "loss": 0.2245,
      "step": 22900
    },
    {
      "epoch": 2.7781132987075736,
      "grad_norm": 1.1361031532287598,
      "learning_rate": 4.1260309900955935e-06,
      "loss": 0.4752,
      "step": 23000
    },
    {
      "epoch": 2.790192052180215,
      "grad_norm": 60.69261169433594,
      "learning_rate": 4.057010732650033e-06,
      "loss": 0.2931,
      "step": 23100
    },
    {
      "epoch": 2.8022708056528565,
      "grad_norm": 14.246147155761719,
      "learning_rate": 3.987990475204473e-06,
      "loss": 0.3326,
      "step": 23200
    },
    {
      "epoch": 2.8143495591254983,
      "grad_norm": 1.4575341939926147,
      "learning_rate": 3.918970217758913e-06,
      "loss": 0.5548,
      "step": 23300
    },
    {
      "epoch": 2.8264283125981398,
      "grad_norm": 105.77811431884766,
      "learning_rate": 3.849949960313352e-06,
      "loss": 0.3421,
      "step": 23400
    },
    {
      "epoch": 2.8385070660707816,
      "grad_norm": 0.2626420855522156,
      "learning_rate": 3.780929702867792e-06,
      "loss": 0.3862,
      "step": 23500
    },
    {
      "epoch": 2.850585819543423,
      "grad_norm": 2.7078840732574463,
      "learning_rate": 3.7119094454222314e-06,
      "loss": 0.4833,
      "step": 23600
    },
    {
      "epoch": 2.8626645730160645,
      "grad_norm": 0.9542246460914612,
      "learning_rate": 3.6428891879766715e-06,
      "loss": 0.3668,
      "step": 23700
    },
    {
      "epoch": 2.8747433264887063,
      "grad_norm": 18.65009307861328,
      "learning_rate": 3.573868930531111e-06,
      "loss": 0.2945,
      "step": 23800
    },
    {
      "epoch": 2.886822079961348,
      "grad_norm": 1.5388355255126953,
      "learning_rate": 3.504848673085551e-06,
      "loss": 0.3557,
      "step": 23900
    },
    {
      "epoch": 2.8989008334339896,
      "grad_norm": 3.0148770809173584,
      "learning_rate": 3.4358284156399906e-06,
      "loss": 0.6596,
      "step": 24000
    },
    {
      "epoch": 2.910979586906631,
      "grad_norm": 37.165157318115234,
      "learning_rate": 3.36680815819443e-06,
      "loss": 0.3057,
      "step": 24100
    },
    {
      "epoch": 2.923058340379273,
      "grad_norm": 5.058041572570801,
      "learning_rate": 3.29778790074887e-06,
      "loss": 0.4866,
      "step": 24200
    },
    {
      "epoch": 2.9351370938519143,
      "grad_norm": 1.483200192451477,
      "learning_rate": 3.2287676433033097e-06,
      "loss": 0.2442,
      "step": 24300
    },
    {
      "epoch": 2.947215847324556,
      "grad_norm": 0.06020691618323326,
      "learning_rate": 3.1597473858577494e-06,
      "loss": 0.3724,
      "step": 24400
    },
    {
      "epoch": 2.9592946007971976,
      "grad_norm": 169.975830078125,
      "learning_rate": 3.090727128412189e-06,
      "loss": 0.3132,
      "step": 24500
    },
    {
      "epoch": 2.9713733542698395,
      "grad_norm": 77.52303314208984,
      "learning_rate": 3.0217068709666293e-06,
      "loss": 0.336,
      "step": 24600
    },
    {
      "epoch": 2.983452107742481,
      "grad_norm": 0.06694803386926651,
      "learning_rate": 2.9526866135210686e-06,
      "loss": 0.4112,
      "step": 24700
    },
    {
      "epoch": 2.995530861215123,
      "grad_norm": 2.1708877086639404,
      "learning_rate": 2.8836663560755083e-06,
      "loss": 0.4656,
      "step": 24800
    },
    {
      "epoch": 3.0,
      "eval_f1_macro": 0.673493442219932,
      "eval_f1_micro": 0.7811492509008154,
      "eval_loss": 0.5058680772781372,
      "eval_roc_auc_macro": 0.985679689545634,
      "eval_runtime": 27.5633,
      "eval_samples_per_second": 600.69,
      "eval_steps_per_second": 18.793,
      "step": 24837
    },
    {
      "epoch": 3.007609614687764,
      "grad_norm": 54.70690155029297,
      "learning_rate": 2.814646098629948e-06,
      "loss": 0.4983,
      "step": 24900
    },
    {
      "epoch": 3.0196883681604056,
      "grad_norm": 5.3064985275268555,
      "learning_rate": 2.7456258411843877e-06,
      "loss": 0.2907,
      "step": 25000
    },
    {
      "epoch": 3.0317671216330475,
      "grad_norm": 11.07393741607666,
      "learning_rate": 2.676605583738828e-06,
      "loss": 0.3143,
      "step": 25100
    },
    {
      "epoch": 3.043845875105689,
      "grad_norm": 3.3939309120178223,
      "learning_rate": 2.607585326293267e-06,
      "loss": 0.2196,
      "step": 25200
    },
    {
      "epoch": 3.055924628578331,
      "grad_norm": 2.940009593963623,
      "learning_rate": 2.5385650688477072e-06,
      "loss": 0.286,
      "step": 25300
    },
    {
      "epoch": 3.0680033820509722,
      "grad_norm": 4.055886268615723,
      "learning_rate": 2.4695448114021465e-06,
      "loss": 0.3013,
      "step": 25400
    },
    {
      "epoch": 3.080082135523614,
      "grad_norm": 11.528753280639648,
      "learning_rate": 2.4005245539565862e-06,
      "loss": 0.2354,
      "step": 25500
    },
    {
      "epoch": 3.0921608889962555,
      "grad_norm": 0.18385331332683563,
      "learning_rate": 2.3315042965110264e-06,
      "loss": 0.6051,
      "step": 25600
    },
    {
      "epoch": 3.1042396424688974,
      "grad_norm": 13.104843139648438,
      "learning_rate": 2.262484039065466e-06,
      "loss": 0.4028,
      "step": 25700
    },
    {
      "epoch": 3.116318395941539,
      "grad_norm": 0.45578253269195557,
      "learning_rate": 2.1934637816199058e-06,
      "loss": 0.2739,
      "step": 25800
    },
    {
      "epoch": 3.1283971494141802,
      "grad_norm": 78.59760284423828,
      "learning_rate": 2.124443524174345e-06,
      "loss": 0.273,
      "step": 25900
    },
    {
      "epoch": 3.140475902886822,
      "grad_norm": 2.6720290184020996,
      "learning_rate": 2.055423266728785e-06,
      "loss": 0.255,
      "step": 26000
    },
    {
      "epoch": 3.1525546563594635,
      "grad_norm": 4.305703163146973,
      "learning_rate": 1.986403009283225e-06,
      "loss": 0.4227,
      "step": 26100
    },
    {
      "epoch": 3.1646334098321054,
      "grad_norm": 4.982120513916016,
      "learning_rate": 1.9173827518376646e-06,
      "loss": 0.291,
      "step": 26200
    },
    {
      "epoch": 3.176712163304747,
      "grad_norm": 1.133713722229004,
      "learning_rate": 1.8483624943921041e-06,
      "loss": 0.3663,
      "step": 26300
    },
    {
      "epoch": 3.1887909167773887,
      "grad_norm": 2.251870632171631,
      "learning_rate": 1.779342236946544e-06,
      "loss": 0.2472,
      "step": 26400
    },
    {
      "epoch": 3.20086967025003,
      "grad_norm": 33.87154006958008,
      "learning_rate": 1.7103219795009837e-06,
      "loss": 0.2784,
      "step": 26500
    },
    {
      "epoch": 3.212948423722672,
      "grad_norm": 7.060152053833008,
      "learning_rate": 1.6413017220554234e-06,
      "loss": 0.2505,
      "step": 26600
    },
    {
      "epoch": 3.2250271771953134,
      "grad_norm": 54.36121368408203,
      "learning_rate": 1.572281464609863e-06,
      "loss": 0.283,
      "step": 26700
    },
    {
      "epoch": 3.2371059306679553,
      "grad_norm": 0.11235402524471283,
      "learning_rate": 1.5032612071643029e-06,
      "loss": 0.3594,
      "step": 26800
    },
    {
      "epoch": 3.2491846841405967,
      "grad_norm": 65.90148162841797,
      "learning_rate": 1.4342409497187426e-06,
      "loss": 0.2565,
      "step": 26900
    },
    {
      "epoch": 3.2612634376132386,
      "grad_norm": 68.92729949951172,
      "learning_rate": 1.3652206922731823e-06,
      "loss": 0.2878,
      "step": 27000
    },
    {
      "epoch": 3.27334219108588,
      "grad_norm": 1.7017191648483276,
      "learning_rate": 1.296200434827622e-06,
      "loss": 0.4109,
      "step": 27100
    },
    {
      "epoch": 3.2854209445585214,
      "grad_norm": 51.373905181884766,
      "learning_rate": 1.2271801773820617e-06,
      "loss": 0.1398,
      "step": 27200
    },
    {
      "epoch": 3.2974996980311633,
      "grad_norm": 0.3657619059085846,
      "learning_rate": 1.1581599199365016e-06,
      "loss": 0.5217,
      "step": 27300
    },
    {
      "epoch": 3.3095784515038047,
      "grad_norm": 1.730783224105835,
      "learning_rate": 1.089139662490941e-06,
      "loss": 0.2631,
      "step": 27400
    },
    {
      "epoch": 3.3216572049764466,
      "grad_norm": 0.6416486501693726,
      "learning_rate": 1.020119405045381e-06,
      "loss": 0.207,
      "step": 27500
    },
    {
      "epoch": 3.333735958449088,
      "grad_norm": 103.94114685058594,
      "learning_rate": 9.510991475998206e-07,
      "loss": 0.2176,
      "step": 27600
    },
    {
      "epoch": 3.34581471192173,
      "grad_norm": 1.8990825414657593,
      "learning_rate": 8.820788901542604e-07,
      "loss": 0.2904,
      "step": 27700
    },
    {
      "epoch": 3.3578934653943713,
      "grad_norm": 3.690469980239868,
      "learning_rate": 8.130586327087e-07,
      "loss": 0.2756,
      "step": 27800
    },
    {
      "epoch": 3.369972218867013,
      "grad_norm": 1.1588436365127563,
      "learning_rate": 7.440383752631399e-07,
      "loss": 0.1938,
      "step": 27900
    },
    {
      "epoch": 3.3820509723396546,
      "grad_norm": 3.9005935192108154,
      "learning_rate": 6.750181178175795e-07,
      "loss": 0.3643,
      "step": 28000
    },
    {
      "epoch": 3.394129725812296,
      "grad_norm": 4.6171650886535645,
      "learning_rate": 6.059978603720193e-07,
      "loss": 0.2291,
      "step": 28100
    },
    {
      "epoch": 3.406208479284938,
      "grad_norm": 4.100077152252197,
      "learning_rate": 5.36977602926459e-07,
      "loss": 0.1147,
      "step": 28200
    },
    {
      "epoch": 3.4182872327575793,
      "grad_norm": 2.583390235900879,
      "learning_rate": 4.679573454808987e-07,
      "loss": 0.2879,
      "step": 28300
    },
    {
      "epoch": 3.430365986230221,
      "grad_norm": 3.452399730682373,
      "learning_rate": 3.989370880353384e-07,
      "loss": 0.4037,
      "step": 28400
    },
    {
      "epoch": 3.4424447397028626,
      "grad_norm": 2.7701127529144287,
      "learning_rate": 3.299168305897781e-07,
      "loss": 0.2061,
      "step": 28500
    },
    {
      "epoch": 3.4545234931755044,
      "grad_norm": 2.339632987976074,
      "learning_rate": 2.6089657314421786e-07,
      "loss": 0.351,
      "step": 28600
    },
    {
      "epoch": 3.466602246648146,
      "grad_norm": 27.174739837646484,
      "learning_rate": 1.9187631569865757e-07,
      "loss": 0.5346,
      "step": 28700
    },
    {
      "epoch": 3.4786810001207877,
      "grad_norm": 14.08242130279541,
      "learning_rate": 1.228560582530973e-07,
      "loss": 0.1511,
      "step": 28800
    },
    {
      "epoch": 3.490759753593429,
      "grad_norm": 2.3419761657714844,
      "learning_rate": 5.3835800807537014e-08,
      "loss": 0.2182,
      "step": 28900
    },
    {
      "epoch": 3.500060393767363,
      "eval_f1_macro": 0.6782633604574576,
      "eval_f1_micro": 0.7839282305783547,
      "eval_loss": 0.5407781600952148,
      "eval_roc_auc_macro": 0.984988684537964,
      "eval_runtime": 27.5563,
      "eval_samples_per_second": 600.843,
      "eval_steps_per_second": 18.798,
      "step": 28977
    }
  ],
  "logging_steps": 100,
  "max_steps": 28977,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0496192751038464e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
